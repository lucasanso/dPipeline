# Pipeline de Dados (ETL)

Neste projeto, busquei compreender e aplicar a metodologia ETL (Extract, Transform and Load) munida com um Pipeline de Dados Escalável. Este projeto desenvolvido com uso de ferramentas: PostgreSQL, AWS (EC2), Docker, Airflow, Snowflake e dbt. Inicialmente, conectei a um banco de dados de uma concessionária fictícia (destinada para prática de ETL), utilizei o serviço de virtualização de máquina da AWS, o EC2, com o sistema operacional Ubuntu. A partir da conexão com a Virtual Machine, através de uma chave SSH, instalei o Docker e reuni as imagens do PostgreSQL e do Airflow em um container para possibilitar o carregamento dos dados da minha máquina física para cloud. Partindo da orquestração do Airflow, por meio de uma DAG, realizei a armazenagem de dados em nuvem no Snowflake. Após isso, utilizando o dbt (Data Build Tool), realizei o tratamento dos dados e gerei análises pós-processamento dos dados brutos, viabilizando uma futura decisão orientada por um analista de dados com dashboards, a exemplo do Power BI.
